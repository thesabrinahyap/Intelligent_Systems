{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Exercie\n",
    "\n",
    "In this exercise, we will apply Feature Selection to a Iris flowers dataset, where the target variable is the Species. Essentially, our goal is to identify the features that are most relevant in discerning the species of each Iris flower. The dataset is from: https://www.kaggle.com/datasets/uciml/iris\n",
    "\n",
    "1. Load the dataset from the exercise's Github Repository (Iris.csv)\n",
    "2. Using buisness logic/common sense, drop features that are surely irrevelvant to the target variable.\n",
    "3. Preprocess your data (split data into training and testing)\n",
    "4. Apply feature selection with the following methods:\n",
    "    - Pearson's correlation coefficient (r)\n",
    "    - Kendall's tau (τ)\n",
    "    - Mutual Information (MI)\n",
    "    - Logistic Regression with L1 penalty\n",
    "6. Compare the results of each feature selection method:\n",
    "    - What features did you manually dropped before applying the feature selection methods? Explain why.\n",
    "    - Are there any common features selected across multiple methods?\n",
    "    - Can you explain why certain features were selected based on their characteristics?\n",
    "(Optional) Visualize the importance of features using techniques like bar charts or heatmaps to make it easier to compare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.stats import pearsonr, kendalltau, mutual_information\n",
    "from statsmodels.api import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection with Pearson's\n",
    "\n",
    "# Calculate correlations between features and target\n",
    "correlations = X_train.corrwith(y_train)\n",
    "\n",
    "# Select top k features with highest absolute correlation\n",
    "k = 3\n",
    "selector = SelectKBest(score_func=correlations.abs(), k=k)\n",
    "selected_features_r = selector.fit(X_train, y_train).get_support(indices=True)\n",
    "\n",
    "print(\"Features selected by Pearson's r:\", X_train.columns[selected_features_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection with Kendall's tau (τ)\n",
    "\n",
    "# Calculate Kendall's tau for each feature with target\n",
    "tau_scores = []\n",
    "for feature in X_train:\n",
    "    tau, _ = kendalltau(X_train[feature], y_train)\n",
    "    tau_scores.append(tau)\n",
    "\n",
    "# Select top k features with highest absolute tau\n",
    "k = 3\n",
    "selector = SelectKBest(score_func=abs, k=k)\n",
    "selected_features_tau = selector.fit(X_train[tau_scores], y_train).get_support(indices=True)\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features selected by Kendall's tau:\", X_train.columns[selected_features_tau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection with Mutual Information (MI)\n",
    "\n",
    "# Calculate Mutual Information for each feature with target\n",
    "mi_scores = []\n",
    "for feature in X_train:\n",
    "    mi = mutual_information(X_train[feature], y_train)\n",
    "    mi_scores.append(mi)\n",
    "\n",
    "# Select top k features with highest mutual information\n",
    "k = 3\n",
    "selector = SelectKBest(score_func=mi_scores, k=k)\n",
    "selected_features_mi = selector.fit(X_train, y_train).get_support(indices=True)\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features selected by Mutual Information:\", X_train.columns[selected_features_mi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection with Logistic Regression with L1 penalty\n",
    "\n",
    "# Fit Logistic Regression with L1 penalty\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients and select non-zero features\n",
    "coefs = model.coef_.flatten()\n",
    "selected_features_lr = X_train.columns[coefs != 0]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features selected by Logistic Regression L1:\", selected_features_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
